
# Continuous Random Variables in R {#R-continuousRV}

Here we investigate in R the common, named continuous random variables we encounter in MATH 340:

- [Uniform probability distribution](#unifR) | `unif`
- [Normal probability distribution](#normalR) |  `norm`
- [Gamma probability distribution](#gammaR) | `gamma`
- [Exponential probability distribution](#expR) | `exp`
- [Chi-square probability distribution](#chiR) | `chisq`
- [Beta probability distribution](#betaR) | `beta`


For each of these distributions we may use the 4 associated commands we used in the discrete case:

- `d___()` gives the density function
- `p___()` gives cumulative probability
- `q___()` gives quantiles
- `r___()` gives random samples

We also discuss below how to build and analyze [homemade continuous random variables in R](#custom-continuous-R).


## Uniform Distribution {#unifR}

The uniform distribution is so very useful, it deserves top-billing here. With it we can generate random numbers, and from it we can build other interesting distributions.

A uniform random variable $X$ over the interval $[a,b]$ has density function $$f(x) = \frac{1}{b-a}, ~~\text{ for all }~~ a \leq x \leq b.$$

### Picking random numbers {-}

```{r}
runif(10,0,1) #pick 10 random numbers between 0 and 1.
```

### Picking random points in the unit square {-}

```{r, fig.dim=c(3,3)}
ggplot(data.frame(x=runif(100,0,1),
                 y=runif(100,0,1)))+
  geom_point(aes(x,y),col="steelblue")+
  theme_bw()
```

### Estimate the value of $\pi$ {-}

```{r, fig.dim=c(4,3)}
points=5000
df <- data.frame(x=runif(points,-1,1),
                 y=runif(points,-1,1))
df$circle <- ifelse(sqrt(df$x^2+df$y^2)<1,"yes","no")
ggplot(df)+
  geom_point(aes(x,y,col=circle),size=.3)+
  xlim(c(-1.1,1.1))+ylim(c(-1.1,1.1))+
  theme_classic()
```

The area of the square is 2*2 = 4.
The area of the circle is $\pi (1)^2 = \pi.$ So the ratio  
$$\text{(area of circle)/(area of square)}=\pi/4,$$
and we can estimate $\pi$ as follows: 
$$\pi \approx 4\cdot \frac{\text{points in circle}}{\text{total points}}$$
```{r}

4*sum(df$circle=="yes")/points # our estimate of pi
```


## Normal Distribution `norm` {#normalR}

Thanks to the Central Limit Theorem this distribution has a central role in statistics.

```{r}
mu=10; sigma=3
x=seq(mu-4*sigma,mu+4*sigma,by=.01)
plot(x,dnorm(x,mu,sigma),type="l",ylab="f(x)")
```

:::{.example #normal-weights-R}

> Suppose newborn birthweights are normally distributed with mean 7.8 pounds and standard deviation 0.95 pounds.  
  a) What proportion of newborns weight more than 10 pounds?  
  b) Find the birth weight that marks the bottom 1% of all birthweights.

```{r}
# part (a)
1-pnorm(10,mean=7.8,sd=0.95)
# part (b)
qnorm(.01,mean=7.8, sd=0.95)
```
:::

### Sampling Distribution of a sample mean {-}

Suppose we have a population of 5000 random numbers between 10 and 20, which should have a uniform looking frequency distribution:
```{r}
pop=runif(5000,10,20)
hist(pop,breaks=15, main="Population Distribution")
```
Now suppose we draw a sample of size 50 from this population, and compute the sample mean of these 50 values:

```{r}
mean(sample(pop,50))
```

Now let's do that game for 10000 trials, and look at the distribution of 10000 sample means:

```{r}
trials=10000
results=c()
for (i in 1:trials){
  results=c(results,mean(sample(pop,50)))
}
hist(results, main="Histogram of sample means")
```

Look Normal?  

```{r}
x=seq(12,18,by=.05)
hist(results, main="Histogram of sample means",freq=FALSE,breaks=30)
curve(dnorm(x,15,10/sqrt(12)/sqrt(50)),add=TRUE)
```


## Exponential Distribution `exp` {#expR}

An exponential random variable $X$ with parameter $\beta$ has pdf $$f(x) = \frac{1}{\beta}e^{-x/\beta} ~~\text{ for }~~ x > 0$$

The **mean** of this distribution is $E(X) = \beta$ and the **rate** associated to this distribution is $1/\beta$.  **In R**, we specify the exponential parameter by entering the rate $1/\beta$, not $\beta$ itself. 

> Suppose $X$ is $\texttt{Exp}(b)$. **In R**, $P(X \leq q)$ is given by `pexp(q,1/b)`  

:::{.example #exp-lightbulb-R} 

The life of a lightbulb is exponentially distributed with mean 120 hours.  

> a) What is the probability that the lightbulb lasts more than 200 hours?  

Here $X$ is exponential with parameter $\beta = 120$. The rate associated with this distribution is $1/120$, so $P(X > 200)$ can be computed with

```{r}
1-pexp(200,rate=1/120)
```

As a reminder, this probability corresponds to the integral
$$\int_{200}^\infty \frac{1}{120}e^{-x/120}~dx$$ which corresponds to the shaded area below


```{r, echo=FALSE,fig.dim=c(6.16,3.4),fig.align='center'}
beta=120;
lb=200; ub=1000
x <- seq(0,1000,.05)
hx <- dexp(x,1/beta)
plot(x, hx, type="n", xlab="", ylab="",ylim=c(0,.01),
     main="", axes=FALSE)
i <- x >= lb & x <= ub
lines(x, hx)
polygon(c(lb,x[i],ub), c(0,hx[i],0), col="seagreen")
abline(h=0, col='#AAAAAA')
#text(x=c(300,lb),y=c(.001,0),c(".189","200"))
axis(1, at=seq(0,800, 200))
axis(2, at=c())
```

> b) What proportion of lightbulbs last fewer than 5 hours?

```{r}
pexp(5,1/120)
```

> c) Find the 5th percentile for this distribution.

```{r}
qexp(.05,1/120)
```

So, 5% of lightbulbs last less than `r round(qexp(.05,1/120),2)` hours.
:::

:::{.example #sketch-exp-f-and-F}

> Suppose $X$ is an exponential random variable with parameter $\beta = 2$.  Sketch the density function $f(x)$ as well as the distribution function $F(x)$.

The density function is $f(x) = \frac{1}{2}e^{-x/2}$ for $x > 0$, and we can sketch it by plotting an $x$ vector of many inputs between, say, 0 and 10, and the corresponding values of `dexp()`:

```{r, echo=FALSE,fig.dim=c(6.16,3.4),fig.align='center'}
beta=2
df = data.frame(x=seq(0,10,by=.01),f=dexp(seq(0,10,by=.01),1/beta))
ggplot(df)+geom_line(aes(x,f),col="red")+
  ggtitle(paste0("density function f(x) of an exp(2) random variable"))+
  ylab("f(x)")+xlab("x")+
  theme_classic()
```

The distribution function, which gives cumulative probability is found by plotting `pexp()`:

```{r, echo=FALSE,fig.dim=c(6.16,3.4),fig.align='center'}
beta=2
df = data.frame(x=seq(0,10,by=.01),f=pexp(seq(0,10,by=.01),1/beta))
ggplot(df)+geom_line(aes(x,f),col="blue")+
  ggtitle(paste0("distribution function F(x) of an exp(2) random variable"))+
  ylab("F(x)")+xlab("x")+
  theme_classic()
```
:::


### A Memoryless distribution {-}

Along with the geometric distribution, the exponential distribution is *memoryless* in this sense: For any $t,s>0$, $$P(X > t + s~|~X > s) = P(X > t).$$

For the geometric distribution we can interpret the above as follows: the probability of waiting more than $t$ trials to see the first success is the same as waiting more than $t$ additional trials after not seeing a success in the first $s$ trials.  

For the "lifetime of a light-bulb interpretation" of the exponential distribution:  However long the light bulb has already lasted, the probability that the light-bulb lasts at least $t$ more hours is the same. 

We can estimate both $P(X>t)$ and $P(X>t+s ~|~ X>s)$ by checking a large random sample from an exponential distribution.

```{r}
trials=10000
x=rexp(trials,rate=1/5)
s=2; t=3
p1=sum(x > t)/trials #P(X > t)
p2=sum(x[which(x > s)]>s+t)/sum(x>s) #P(X>t+s | X > s)
print(paste("Estimate for P(X>t):",round(p1,3)))
print(paste("Estimate for P(X>t+s | X>s):",round(p2,3)))
```

## Gamma Distribution `gamma` {#gammaR}

Recall, the gamma probability distribution, $\texttt{gamma}(\alpha,\beta)$ is a family of skeyed right distributions. The parameter $\alpha$ is sometimes called the **shape** parameter, $\beta$ is called the **scale** parameter, and its reciprocal $1/\beta$ is called the **rate**.  Figure \@ref(fig:3-gamma-plots) plots 3 different gamma density functions. In R we refer to a gamma distribution in our `p_`,`q_`,`d_`, and `r_` functions via the shape parameter $\alpha$ and either the rate $1/\beta$ or the  scale $\beta$ parameter. It's good practice to label the inputs. 

> Suppose $X$ is $\texttt{gamma}(a,b)$. In R  $P(X \leq q)$ is given by `pgamma(q,shape=a,rate=1/b)` or `pgamma(q,shape=a,scale=b)` or `pgamma(q,a,1/b)` (if you don't label the two parameters, R assumes (shape,rate)).

:::{.example #gamma-R}

Suppose $X$ has a gamma distribution with parameters $\alpha=3$ and $\beta = 4$.  

> a) Find $P(4 < X < 12)$.

This probability corresponds to the area pictured in Figure \@ref(fig:gamma-area), and can be computed in R, remembering to input the shape parameter $\alpha$ and the rate parameter $1/\beta$:


```{r}

pgamma(12,shape=3,scale=4)-pgamma(4,shape=3,scale=4)
```

Just about a 50% chance that a random value from a $\texttt{gamma}(3,4)$ distribution is between 4 and 12.

```{r, echo=FALSE,label="gamma-area",fig.dim=c(4,3),fig.align='center',fig.cap="Finding P(4<X<12) for a gamma(3,4) distribution"}
alpha=3; beta=4
lb=4; ub=12
x <- seq(0,20,.05)
hx <- dgamma(x,alpha,1/beta) #assumes (x,shape,rate) if no labels
plot(x, hx, type="n", xlab="", ylab="",
     main="", axes=FALSE)
i <- x >= lb & x <= ub
lines(x, hx)
polygon(c(lb,x[i],ub), c(0,hx[i],0), col="seagreen")
abline(h=0, col='#AAAAAA')
#text(x=c(300,lb),y=c(.001,0),c(".189","200"))
axis(1, at=seq(0,20,4))
axis(2, at=c())

```


> b) Gather a random sample of 1000 values from this distribution, and determine what proportion of them live between 4 and 12.

```{r}
x=rgamma(1000,3,1/4) # random sample of size 1000 (no parameter names <-> shape,rate)
sum(abs(x-8)<4) # values in the sample between 4 and 12
```

Well, `r sum(abs(x-8)<4)` is mighty close to half of the 1000 values!
:::

## Beta distribution `beta` {#betaR}

The beta$(\alpha,\beta)$ probability distribution provides a way to model random variables whose possible outcomes are all real numbers between 0 and 1.  Such distributions are useful for modeling proportions.  As with the gamma and normal distributions, this is a 2-parameter family of distributions.  

:::{.example #beta-sales-R}
Let $X$ denotes the proportion of sales on a particular website that comes from new customers any given day, and suppose from past experience, $X$ is well-modeled with a beta distribution with shape parameters $\alpha = 1$, and $\beta=3.5$.  

> Determine the probability that on any given day, over 1/2 the sales come from new customers.

```{r}
1-pbeta(0.5,1,3.5)
```
:::


## Homemade Continuous Random Variables {#custom-continuous-R}

We may wish to study a continuous random variable $X$ from a given probability density function such as $f(x) = \frac{3}{8}(2-x)^2$ for $0 \leq x \leq 2$.

In this case, probabilities such as $P(X > 1.2)$ correspond to areas under the density curve, which are calculated by integration, e.g., $$P(X > 1.2) = \int_{1.2}^2 \frac{3}{8}(2-x)^2~dx.$$

If we can find an antiderivative of $f(x)$, we can find this probability using the fundamental theorem of calculus.  If not, we can always estimate the value of the integral with Riemann sums.  We do this below.

### Input the density function {-}

First build the probability density function (pdf) as a function in R.

```{r,label="define pdf"}
f_pdf <- function(x){
  return(3*(2-x)^2/8)
  }
```

### Visualize the density function {-}

We create a vector of inputs `x` going from 0 to 2 in small increments (the increment is .01 below), to give us many points over the interval of interest [0,2].  Then we **plot the density curve** by plotting these x values against the function values f(x).  (`type="l"` gives us a **l**ine plot instead of a point plot).


```{r,label="plot pdf"}
x=seq(0,2,by=.01)
plot(x,f_pdf(x),type="l",
     main="the density function")
```

### Estimating Integrals with Riemann Sums {-}

We know $P(X \geq 1.2)$ corresponds to the area under the density curve between 1.2 and 2.  We can estimate areas by computing a Riemann Sum (a sum of many thin rectangle areas approximating the area under the density curve).

Here's a function for estimating $\int_a^b f(x)~dx$ with a sum of $n$ rectangle areas, generated using the midpoint rule.

```{r,label="define midpoint sum"}
mid_sum=function(f,a,b,n){
  #inputs:
      #f - function
      #a, b - lower and upper bounds of interval
      #n - number of subdivisions
  #output: The sum of the n rectangle areas whose heights are
  # determined by the midpoint rule
  dx=(b-a)/n
  ticks=seq(a+dx/2,b,dx)
  return(sum(f(ticks)*dx))
}
```

For instance, `mid_sum(f_pdf,a=0.4,b=1.2,n=4)` computes the area of the 4 rectangles in Figure \@ref(fig:midpoint-sum).  We divide the interval [0.4,1.2] into n=4 equal-width subintervals, and build rectangles having height equal to the function height at the midpoint of each of these subintervals.

```{r,echo=FALSE,label="midpoint-sum",fig.dim=c(4,3)}
x1=c(seq(0,.399,.001),seq(.4,.599,.001),seq(.6,.799,.001),
     seq(.8,.999,.001),seq(1,1.2,.001),seq(1.201,2,.001))
y1=c(rep(0,400),rep(f_pdf(.5),200),rep(f_pdf(.7),200),rep(f_pdf(.9),200),
     rep(f_pdf(1.1),201),rep(0,800))
df <- data.frame(x=x1,y=y1,f=f_pdf(x1))
ggplot()+
  geom_line(data=df,aes(x,f))+
  geom_area(data=df,aes(x,y),col="white",fill="steelblue",alpha=.6)+
  geom_segment(aes(x=0.59,y=0,xend=0.59,yend=f_pdf(0.5)),col="white")+
  geom_segment(aes(x=0.79,y=0,xend=0.79,yend=f_pdf(0.7)),col="white")+
  geom_segment(aes(x=0.99,y=0,xend=0.99,yend=f_pdf(0.9)),col="white")+
  geom_segment(aes(x=1.2,y=0,xend=1.2,yend=f_pdf(1.1)),col="white")+
    geom_segment(aes(x=0.5,y=0,xend=0.5,yend=f_pdf(0.5)),col="red",linetype="dashed")+
  geom_segment(aes(x=0.7,y=0,xend=0.7,yend=f_pdf(0.7)),col="red",linetype="dashed")+
  geom_segment(aes(x=0.9,y=0,xend=0.9,yend=f_pdf(0.9)),col="red",linetype="dashed")+
  geom_segment(aes(x=1.1,y=0,xend=1.1,yend=f_pdf(1.1)),col="red",linetype="dashed")+
  scale_x_continuous(breaks=c(0.4,0.6,0.8,1.0,1.2))+
  geom_hline(yintercept=0,size=.5)+
  theme(panel.grid.major=element_blank(),panel.grid.minor=element_blank())
  
```

The area of the four rectangles is
```{r,label="mid sum example"}
mid_sum(f_pdf,a=.4,b=1.2,n=4)
```


### Estimating Probabilities {-}

So, getting back to our example, if we want to estimate $P(X > 1.2)$ we can compute a midpoint sum - the more rectangles the better.  Let's start with $n = 100$:

```{r,label="mid sum example 2a"}
mid_sum(f_pdf,1.2,2,100)
```

What if we use $n = 1000$ rectangles?


```{r,,label="mid sum example 2b"}
mid_sum(f_pdf,1.2,2,1000)
```

It seems as if our estimate hasn't changed much by going from 100 to 1000 subintervals, for this density function.

To estimate $P(0.5 < X < 1.1)$ we can evaluate

```{r}
mid_sum(f_pdf,0.5,1.1,100)
```

### The distribution function $F(X)$ {-}

Recall, $F(x)$ gives cumulative probability.  In particular, $F(x) = P(X \leq x)$.

Consider again the random variable $X$ with pdf $f(x) = (3/8)(2-x)^2$ for $0 < x < 2$.  

For any value of $b$ between 0 and 2, $$F(b) = \int_0^b f(x)~dx,$$
which we can numerically approximate with

```{r,label="distribution-function-homemade"}
F_example <- function(b){
  return(mid_sum(f_pdf,0,b,100))
}
```

Then we can sketch the graph of the distribution function, for inputs between 0 and 2


```{r,label="plot distn fcn"}
x=seq(0,2,by=.01)
y=c()
for (i in 1:length(x)){
  y=c(y,F_example(x[i]))
}
plot(x,y,type="l",
     main="the distribution function")
```


### Estimating Moments {-}

Recall, $E(X^n)$ is called the *$n$th moment about 0* of the distribution.  The first moment is the expected value $E(X)$, and the 2nd and 1st together determine the variance: $V(X) = E(X^2)-E(X)^2.$

For a continuous random variable $X$ with pdf $f(x)$, $$E(X^n) = \int_{-\infty}^\infty x^n \cdot f(x).$$

In R we can numerically estimate these integrals with the `mid_sum()` function defined above, applied to the integrand $x^n\cdot f(X)$.  

```{r,label="moment function"}
moment.integrand<-function(f,n){
  #inputs:
      # f - a previously defined pdf
      # n - an integer
  #output: the integrand function for evaluating E(X^n)
  return(function(x){return(x^n*f(x))})
}
```


### Expected Value {-}

For a continuous random variable $X$, $$E(X)=\int_{-\infty}^{\infty} x \cdot f(x)~dx.$$

To estimate this integral, we plug the first moment integrand $x \cdot f(x)$ into our Riemann sum function.
```{r}
mu=mid_sum(moment.integrand(f_pdf,1),0,2,100)
mu
```

Note: The actual expected value is

$$\int_0^2 x \cdot (3/8)(2-x)^2~dx = 0.5.$$

We estimate the **variance** knowing that $V(X) = E(X^2)-E(X)^2.$ 

```{r}
EX2=mid_sum(moment.integrand(f_pdf,2),0,2,100)
EX2
```

So the variance of $X$ is
```{r}
EX2-mu^2
```

Note: The actual value of $E(X^2)$ is $$\int_0^2 x^2 \cdot (3/8)(2-x)^2~dx = 0.4,$$
so $V(X) = 0.4 - (0.5)^2 = 0.15.$
